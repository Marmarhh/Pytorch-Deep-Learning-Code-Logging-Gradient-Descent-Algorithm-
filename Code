#批量梯度下降
#任务要求:
#利用批量梯度下降算法，用y=wx拟合
#通过数据x_data = [1.0, 2.0, 3.0]，y_data = [2.0, 4.0, 6.0]进行训练，找出最佳w
#并可视化输出每一阶段的Loss和w组成的Loss-w图,及Loss随训练次数的变化的图Loss-epoch

import matplotlib.pyplot as plt

# 训练数据集
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

#初始猜测的权重
w=1.0

def forward(x,w):
    return x*w

#单个点对应的损失函数
def loss(x,y,w):
    y_pred = forward(x,w)
    return (y_pred -y)**2

#所有点总的损失函数
def Loss(x_set,y_set,w): #x_set,y_set指代数据集，由于不能重名故不用x_data ，y_data
    Loss1= 0
    for x,y in zip(x_set,y_set):
        Loss1 += loss(x,y,w)
    return Loss1/ len(x_set)

#梯度函数(Loss的对w的导数,由于Loss是求和了的故该倒数不是loss对w的导数,不是单个的）
def gradient(x_set,y_set,w):
    grad =0
    for x,y in zip(x_set,y_set):
        grad += 2*x*(x*w-y)
    return grad / len(x_set)

Loss_list=[]
w_list=[]
epoch_list=[]
learning_rate = 0.01 #学习率

#进行训练,更改w100次
for epoch in range(100):
    Loss_val =Loss(x_data,y_data,w)
    gradient_val =gradient(x_data,y_data,w)
    w_list.append(w)
    Loss_list.append(Loss_val)
    epoch_list.append(epoch)
    w -= learning_rate*gradient_val

print(Loss_list)
print(w_list)
print(epoch_list)

# 可视化 Loss 随训练次数epoch的变化
plt.plot(epoch_list,Loss_list,label='Loss-epoch')
plt.ylabel('Loss')
plt.xlabel('epoch')
plt.legend()
# 自动添加图例:在绘图时使用了label，如plt.plot(x, y, label='数据标签') 
# 调用 plt.legend() 时，Matplotlib 会自动根据这些标签创建图例
plt.show()

# 可视化 Loss 和 w 的关系
plt.plot(w_list,Loss_list, label='Loss-w')
plt.ylabel('Loss')
plt.xlabel('w')
plt.legend()
plt.show()



#随机梯度下降
#任务要求:
#利用随机梯度下降算法，用y=wx拟合
#通过数据x_data = [1.0, 2.0, 3.0]，y_data = [2.0, 4.0, 6.0]进行训练，找出最佳w
#并可视化输出每一阶段的loss和w组成的loss-w图,及loss随训练次数的变化的图loss-epoch

import matplotlib.pyplot as plt

# 训练数据集
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

#初始猜测的权重
w=1.0

def forward(x,w):
    return x*w

#单个点对应的损失函数
def loss(x,y,w):
    y_pred = forward(x,w)
    return (y_pred -y)**2

#单点梯度函数(loss的对w的导数,是单点loss，不是总的Loss）
def gradient(x,y,w):
    return 2*x*(x*w - y)

loss_list=[]
w_list=[]
epoch_list=[]
learning_rate = 0.01 #学习率

#进行训练,更改w100*3=300次
for epoch in range(100):
    for x,y in zip(x_data,y_data):
        gradient_val=gradient(x,y,w)
        loss_val=loss(x,y,w)
        w-=learning_rate*gradient_val
    epoch_list.append(epoch)
    loss_list.append(loss_val)
    w_list.append(w)
    
print(loss_list)
print(w_list)
print(epoch_list)
        
# 可视化 loss 随训练次数epoch的变化
plt.plot(epoch_list,loss_list,label='loss-epoch')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend()
plt.show()

# 可视化 loss 和 w 的关系
plt.plot(w_list,loss_list, label='loss-w')
plt.ylabel('loss')
plt.xlabel('w')
plt.legend()
plt.show()
